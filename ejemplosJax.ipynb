{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6752311f",
   "metadata": {},
   "source": [
    "\n",
    "### JAX\n",
    "\n",
    "https://github.com/jax-ml/jax\n",
    "\n",
    "JAX es una biblioteca de Python para el cálculo de matrices orientado a aceleradores y la transformación de programas, diseñada para el cálculo numérico de alto rendimiento y el aprendizaje automático a gran escala.\n",
    "\n",
    "JAX puede diferenciar automáticamente funciones nativas de Python y NumPy. Puede diferenciar mediante bucles, ramas, recursión y cierres, y puede obtener derivadas de derivadas de derivadas. Admite la diferenciación en modo inverso (también conocida como retropropagación) jax.grady la diferenciación en modo directo, y ambas pueden componerse arbitrariamente en cualquier orden.\n",
    "\n",
    "JAX usa XLA para compilar y escalar tus programas NumPy en TPU, GPU y otros aceleradores de hardware. Puedes compilar tus propias funciones puras con jax.jit. La compilación y la diferenciación automática se pueden componer arbitrariamente.\n",
    "\n",
    "Profundice un poco más y verá que JAX es realmente un sistema extensible para transformaciones de funciones componibles a escala .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca9a31",
   "metadata": {},
   "source": [
    "### Documentación \n",
    "\n",
    "https://docs.jax.dev/en/latest/\n",
    "\n",
    "### Guía del desarrollador\n",
    "\n",
    "https://docs.jax.dev/en/latest/developer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aac58a",
   "metadata": {},
   "source": [
    "### Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6026171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Función de predicción \n",
    "def predict(params, inputs):\n",
    "  for W, b in params:\n",
    "    outputs = jnp.dot(inputs, W) + b\n",
    "    inputs = jnp.tanh(outputs)  # inputs to the next layer\n",
    "  return outputs                # no activation on last layer\n",
    "\n",
    "# Función de pérdida\n",
    "def loss(params, inputs, targets):\n",
    "  preds = predict(params, inputs)\n",
    "  return jnp.sum((preds - targets)**2)\n",
    "\n",
    "# grad_loss calcula el gradiente de la función de pérdida con respecto a los parámetros del modelo.\n",
    "# compilado con jax.jit para optimizar su rendimiento.\n",
    "# Esto permite que la función se ejecute de manera más eficiente, especialmente en cálculos repetitivos.\n",
    "# La función jax.grad se utiliza para obtener el gradiente de la función de pérdida.\n",
    "# Esto es esencial para algoritmos de optimización que ajustan los parámetros del modelo para minimizar\n",
    "# la pérdida.\n",
    "grad_loss = jax.jit(jax.grad(loss))  \n",
    "\n",
    "# perex_grads calcula los gradientes de la función de pérdida para cada ejemplo en un lote de datos.\n",
    "# Utiliza jax.vmap para vectorizar la función grad_loss, permitiendo así el cálculo eficiente de los \n",
    "# gradientes\n",
    "# cálculo rápido y paralelo de los gradientes para múltiples ejemplos de entrada y sus correspondientes\n",
    "# objetivos.\n",
    "perex_grads = jax.jit(jax.vmap(grad_loss, in_axes=(None, 0, 0)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc5a4d",
   "metadata": {},
   "source": [
    "### Transformaciones\n",
    "\n",
    "En esencia, JAX es un sistema extensible para transformar funciones numéricas. Aquí hay tres: jax.grad, jax.jit, y jax.vmap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81570fd2",
   "metadata": {},
   "source": [
    "**Diferenciación automática con grad**\n",
    "\n",
    "Uso de jax.grad para calcular eficientemente gradientes de modo inverso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01326a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4199743\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def tanh(x):\n",
    "  y = jnp.exp(-2.0 * x)\n",
    "  return (1.0 - y) / (1.0 + y)\n",
    "\n",
    "grad_tanh = jax.grad(tanh)\n",
    "print(grad_tanh(1.0))\n",
    "# prints 0.4199743"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa168e",
   "metadata": {},
   "source": [
    "Diferenciamos cualquier pedido con grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51754028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6216267\n"
     ]
    }
   ],
   "source": [
    "print(jax.grad(jax.grad(jax.grad(tanh)))(1.0))\n",
    "# prints 0.62162673"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adbc1f6",
   "metadata": {},
   "source": [
    "Podemos usar la diferenciación con el flujo de control de Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3cad6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "def abs_val(x):\n",
    "  if x > 0:\n",
    "    return x\n",
    "  else:\n",
    "    return -x\n",
    "\n",
    "abs_val_grad = jax.grad(abs_val)\n",
    "print(abs_val_grad(1.0))   # prints 1.0\n",
    "print(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db7944",
   "metadata": {},
   "source": [
    "**Recopilación con jit**\n",
    "\n",
    "Utilice XLA para compilar sus funciones de extremo a extremo con jit, utilizado como @jitdecorador o como función de orden superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2167137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 312.96 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "3.54 ms ± 4.95 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\n",
      "232 ms ± 21.7 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def slow_f(x):\n",
    "  # Element-wise ops see a large benefit from fusion\n",
    "  return x * x + x * 2.0\n",
    "\n",
    "x = jnp.ones((5000, 5000))\n",
    "fast_f = jax.jit(slow_f)\n",
    "%timeit -n10 -r3 fast_f(x)\n",
    "%timeit -n10 -r3 slow_f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147f4db",
   "metadata": {},
   "source": [
    "El uso jax.jitrestringe el tipo de flujo de control de Python que la función puede usar; consulte el tutorial sobre Flujo de control y operadores lógicos con JIT para obtener más información."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f05766",
   "metadata": {},
   "source": [
    "**Autovectorización convmap**\n",
    "\n",
    "vmapMapea una función a lo largo de los ejes de una matriz. Pero en lugar de simplemente recorrer las aplicaciones de la función, lo desplaza hacia las operaciones primitivas de la función; por ejemplo, convierte las multiplicaciones matriz-vector en multiplicaciones matriz-matriz para un mejor rendimiento.\n",
    "\n",
    "El uso vmappuede ahorrarle el tener que llevar dimensiones de lote en su código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87c04a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def l1_distance(x, y):\n",
    "  assert x.ndim == y.ndim == 1  # only works on 1D inputs\n",
    "  return jnp.sum(jnp.abs(x - y))\n",
    "\n",
    "def pairwise_distances(dist1D, xs):\n",
    "  return jax.vmap(jax.vmap(dist1D, (0, None)), (None, 0))(xs, xs)\n",
    "\n",
    "xs = jax.random.normal(jax.random.key(0), (100, 3))\n",
    "dists = pairwise_distances(l1_distance, xs)\n",
    "dists.shape  # (100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae5a623",
   "metadata": {},
   "source": [
    "Al componer jax.vmapcon jax.grady jax.jit, podemos obtener matrices jacobianas eficientes, o gradientes por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b14d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_example_grads = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e60e4",
   "metadata": {},
   "source": [
    "**Escalada**\n",
    "\n",
    "Consultar\n",
    "\n",
    "https://github.com/jax-ml/jax?tab=readme-ov-file#scaling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CondaCodingQiskitEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
